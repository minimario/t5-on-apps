import numpy as np
import re
from datasets import Dataset
from random import getrandbits
from codet5_scored_partials import *
import json
import tqdm
from typing import List, Tuple

def listify(obj):
    # Recursively flattens a results array into a list
    if isinstance(obj, bool) or isinstance(obj, int):
        return obj
    if isinstance(obj, np.ndarray):
        return listify(obj.tolist())
    return [listify(i) for i in obj]

def score_program(dataset, i, split, save_path):
    scored_path = f"{save_path}/{split}_{i}_scored.json"
    if os.path.exists(scored_path):
        return

    sample = dataset[i]
    try:
        candidates = json.load(open(f"{save_path}/{split}_{i}.json"))
        results = evaluate_generations_problem(candidates, sample, debug=False, verbose=False)
        score = get_percentage_correct(results)
        save_json = {"candidates": candidates, "results": results, "score": score}
        try:
            with open(f"{save_path}/{split}_{i}_scored.json", "w") as f:
                json.dump(save_json, f)
        except:
            # for example, if there's a numpy array somewhere in there
            results = listify(results)
            save_json = {"candidates": candidates, "results": results, "score": score}
            with open(f"{save_path}/{split}_{i}_scored.json", "w") as f:
                json.dump(save_json, f)
            
    except Exception as e:
        pass

def make_candidate_list_from_dataset(dataset : Dataset) -> List[dict]:
    keys = list(dataset.features.keys())
    key_lists = {key: dataset[key] for key in keys}
    data = []
    for i in range(len(dataset)):
        data_json = {}
        for key in keys:
            data_json[key] = key_lists[key][i]
        data.append(data_json)
        # data_json = {"prob_id": dataset[i]["prob_id"],
        #             "prompt": dataset[i]["prompt"],
        #             "completion": dataset[i]["completion"],
        #             "binary_label": dataset[i]["binary_label"],
        #             "ternary_label": dataset[i]["ternary_label"]}
    return data
        
def make_candidate_list(dataset : Dataset, split : str, save_path : str, difficulties = []) -> List[dict]:
    # Given dataset, split, save_path, and difficultues
    # Makes a list of dictionaries, each containing
    # prob_id: problem id
    # prompt: the question statement
    # candidate: the solution (generated by T5)
    # binary_label: 1 if correct, 0 if incorrect
    # ternary_label: Correct, Intent error, or Execution error
    data = []
    for i in range(len(dataset)):
        if dataset[i]["difficulty"] not in difficulties:
            continue
        try:
            scored_path = f"{save_path}/{split}_{i}_scored.json"
            with open(scored_path, "r") as f:
                save_json = json.load(f)
            
            candidates = save_json["candidates"]
            results = save_json["results"]
            if len(candidates) != len(results):
                continue
            results_ternary = get_per_candidate_results(results)
            results_binary = ternary_to_binary(results_ternary)

            prompt = format_problem_str(dataset[i], answer_type_preprocessing="jeevana")
            for j in range(len(candidates)):
                data_json = {"prob_id": f"{split}_{i}",
                            "prompt": prompt,
                            "completion": candidates[j],
                            "binary_label": results_binary[j],
                            "ternary_label": results_ternary[j]}
                data.append(data_json)
        except:
            print("Failed sample", i)

    return data

def make_candidate_list_partials(dataset : Dataset, split : str, save_path : str, difficulties):
    data = []
    candidates = make_candidate_list(dataset, split, save_path, difficulties)
    for candidate in tqdm.tqdm(candidates):
        completion = candidate["completion"]
        if candidate["binary_label"] == "Correct":
            completion_with_partials = get_partials(completion, num_partials=5, min_frac=0.3)
        else:
            completion_with_partials = get_partials(completion, num_partials=5, min_frac=0.3)

        for completion_partial, slice_id, percentage in completion_with_partials:
            data_json = {"prob_id": candidate["prob_id"],
                        "prompt": candidate["prompt"],
                        "completion": completion_partial, 
                        "slice_id": slice_id,
                        "percentage": percentage,
                        "binary_label": candidate["binary_label"],
                        "ternary_label": candidate["ternary_label"]}
            data.append(data_json)
    return data

def make_json_file_full(dataset : Dataset, split : str, save_path : str, json_file_path : str, difficulties = []):
    candidates = make_candidate_list(dataset, split, save_path, difficulties)
    data = [json.dumps(i) for i in candidates]
    open(json_file_path, "w").write('\n'.join(data))

def make_json_file_partials(dataset : Dataset, split : str, save_path : str, json_file_path : str, difficulties = []):
    data = []
    candidates = make_candidate_list_partials(dataset, split, save_path, difficulties)
    data = [json.dumps(i) for i in candidates]
    open(json_file_path, "w").write('\n'.join(data))

# def get_partials(code, num_partials=10, min_frac=0.5):
#     if num_partials == 1:
#         return [code]
#     code = code.split("\n")

#     if num_partials == -1:
#         partial_line_counts = range(max(1, int(len(code)*min_frac)), len(code)+1) 
#     else:
#         fracs = np.linspace(min_frac, 1, num_partials)
#         partial_line_counts = [max(1, int(len(code)*frac)) for frac in fracs]
#         partial_line_counts = list(set(partial_line_counts)) # dedup
#     return ['\n'.join(code[0:i]) for i in partial_line_counts]

def get_partials(code : str, num_partials=10, min_frac=0.5) -> List[Tuple[str, int, float]]:
    if num_partials == 1:
        return [code]

    matches = []
    for keyword in ["if", "for", "else", "\n"]:
        matches += [match.start() for match in re.finditer(keyword, code)]
    matches = [i for i in matches if i == 0 or code[i-1] not in '\t\n']
    matches.sort()

    min_char_count = max(1, int(len(code)*min_frac))
    matches = [match for match in matches if match >= min_char_count]
    if len(matches) == 0:
        return []
    if num_partials == -1:
        partial_line_counts = matches
    else:
        partial_indices = []
        for i in range(num_partials):
            match_index = int((len(matches)-1)*i/(num_partials-1))
            partial_indices.append(matches[match_index])
        partial_line_counts = list(set(partial_indices))
        partial_line_counts.sort()
    
    partials = []
    for i, lc in enumerate(partial_line_counts):
        partials.append((code[0:lc], i, int(lc/len(code)*100))) 
    return partials

    # partials = [(code[0:i], int(i/len(code)*100)) for i in partial_line_counts]
    # import random
    # if random.randint(1, 10000) == 1:
    #     print(code)
    #     print("-"*10)
    #     print("PARTIALS")
    #     for p in partials:
    #         print(p)
    #         print("--")
    #     input()


def f(i, dataset, split, save_path):
    score_program(dataset, i, split, save_path)
    return

def score_generated_programs(dataset, split, save_path):
    from concurrent.futures import ProcessPoolExecutor
    with ProcessPoolExecutor(max_workers=64) as executor:
        for i in range(len(dataset)):
            executor.submit(f, i, dataset, split, save_path)
    executor.shutdown()
    # for result in executor.map(f, range(len(dataset)), chunksize=120):
    #     print(result)

def get_slice_num(percentage):
    if percentage < 50: return 0
    elif percentage < 70: return 1
    elif percentage < 90: return 2
    else: return 3

def create_val_grouped_files_partial(dataset):
    grouped_indices = {}
    grouped_labels = {}
    for i in tqdm.trange(len(dataset["val"])):
        sample = dataset["val"][i]
        id = sample["prob_id"]
        slice = get_slice_num(sample["percentage"])
        label = sample["ternary_label"]
        if (id, slice) not in grouped_indices:
            grouped_indices[(id, slice)] = []
            grouped_labels[(id, slice)] = []
        grouped_indices[(id, slice)].append(i)
        grouped_labels[(id, slice)].append(label)

    max_length = -1
    for i in grouped_indices.values():
        max_length = max(max_length, len(i))

    for i in grouped_indices.keys():
        while len(grouped_indices[i]) < max_length:
            grouped_indices[i].append(-1)
            grouped_labels[i].append(-1)

    sorted_keys = list(grouped_indices.keys())
    sorted_keys.sort(key = lambda x:x[1])
    counts = {0: 0, 1: 0, 2: 0, 3: 0}
    for (_, count) in sorted_keys:
        counts[count] += 1
    print("Counts: ", counts)

    sorted_index_values = []
    sorted_label_values = []
    for key in sorted_keys:
        sorted_index_values.append(grouped_indices[key])
        sorted_label_values.append(grouped_labels[key])

    val_grouped_indices = np.array(sorted_index_values)
    val_grouped_labels = np.array(sorted_label_values)
    np.save("val_grouped_indices.npy", val_grouped_indices)
    np.save("val_grouped_labels.npy", val_grouped_labels)

def create_val_grouped_files(dataset):
    grouped_indices = {}
    grouped_labels = {}
    for i in range(len(dataset["val"])):
        sample = dataset["val"][i]
        id = sample["prob_id"]
        label = sample["ternary_label"]
        if id not in grouped_indices:
            grouped_indices[id] = []
            grouped_labels[id] = []
        grouped_indices[id].append(i)
        grouped_labels[id].append(label)

    max_length = -1
    for i in grouped_indices.values():
        max_length = max(max_length, len(i))
    for i in grouped_indices.keys():
        while len(grouped_indices[i]) < max_length:
            grouped_indices[i].append(-1)
            grouped_labels[i].append(-1)

    val_grouped_indices = np.array(list(grouped_indices.values()))
    val_grouped_labels = np.array(list(grouped_labels.values()))
    np.save("val_grouped_indices.npy", val_grouped_indices)
    np.save("val_grouped_labels.npy", val_grouped_labels)
    
if __name__ == "__main__":
    train_split = "train"
    train_save_path = "full_programs_t5_train"

    val_split = "val" 
    val_save_path = "full_programs_t5_val"
    num_partials = 1

    # # load the right dataset
    # train_dataset_all, train_dataset_with_tests, val_dataset, test_dataset = load_datasets()
    # dataset_dict = {"train": train_dataset_with_tests, "val": val_dataset, "test": test_dataset}
    # train_dataset = dataset_dict[train_split]
    # val_dataset = dataset_dict[val_split]

    # Step 0: Run codet5_scored_partials.py to generate the programs

    # Step 1: Score the generated programs
    # score_generated_programs(dataset, split, save_path)

    # Step 2: Make the JSON file
    # Looks at scored json files, splits each program into partial programs
    # and then writes everything to one json file. 
    # make_json_file_full(train_dataset_with_tests, train_split, train_save_path, "train_full_introductory.json", difficulties=["introductory"])
    # make_json_file_full(val_dataset, val_split, val_save_path, "val_full_introductory.json", difficulties=["introductory"])
    # make_json_file_partials(train_dataset_with_tests, train_split, train_save_path, "train_partials_introductory.json", difficulties=["introductory"])
    # make_json_file_partials(val_dataset, val_split, val_save_path, "val_partials_introductory.json", difficulties=["introductory"])

    # Step 3: Make the grouped indices and labels
    # VAL ONLY: Creates the grouped indices and labels necessary for the CodeRanker evaluation
    # dataset_dict_new = {"val": f"partial_{num_partials}_ranker_data_{split}.json"}
    dataset_dict_new = {"val": "val_acc_metric.json"}
    dataset_new = load_dataset("json", data_files=dataset_dict_new, cache_dir="./cache")
    # dataset_new = dataset_new.shuffle(seed=42)
    create_val_grouped_files_partial(dataset_new)
